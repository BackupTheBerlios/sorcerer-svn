Until 2013 most POSIX provided
only a single root or real file system.
The introduction of unshared namespace and
virtual Ethernet devices into the Linux kernel
made possible the potential for creating
a POSIX with multiple real file systems.

GUID partition tables allow approximately 127 partitions.
Linux provides by default 64 virtual consoles.
About 60 real file systems could exist.
Testing involved no more than 6.
Each real file system is mounted
on an immediate sub directory of /+/
Every deployment requires a minimum of 2 real file systems.

/+/base/ is the real file system common to all deployments.
The base real file system provides installed software
suitable for creating initramfs images and updating.
initramfs provide the files that populate the rootfs.

Other real file systems can be created
that are specific to the role being performed.
Real file systems on a server might be:
/+/base/
/+/dns/
/+/ftp/
/+/mail/
/+/test/
/+/www/

Each contains both role specific software
and the software required for updating.
Each is not unlike a chroot jail.
Intrusion in one file system
might not compromise all file systems.

If immune-security-tomoyo is also deployed
then tomoyo can confine interlopers.
Escape could be very difficult.
Within a restricted tomoyo domain;
EUID 0 processes can view as much as expected,
yet are unable to modify areas such as:
/etc/ /bin/ /sbin/ /usr/ /opt/

While able to exploit a current vulnerability
in order to gain unauthorized access;
tomoyo domains could prevent the interloper
from installing a permanent backdoor.
Consequently, an update could fix a vulnerability
which prohibit the interloper's continued unauthorized access.

At least one computer on a LAN should be a test computer
Or a /+/test/ can be created.
The manifest for /+/test/
should including all installed software
within each real file system.
/+/test/ should be updated first.

Problems with updates can be observed.
Services expected responses can be verified.
Encountering surprises on a /+/test/
allows problems to be anticipated and avoided
on production servers.

/+/www/ is obviously where the web server runs.
However, if vhosting
then the possibility exists where
each FQDN can receive it's own file systems.
However, that requires additional IP address
since vhosting typically associates multiple FQDNs
with a single IP address.

/+/dns/ could be the place
where an authoritative name server for a domain runs.

A possible configuration is:
/+/base/ ;
/+/work/ ;
/+/user/

/+/work/ could contain company approved software.
/+/work/ could be intended for use on work related tasks only.
However, a desire to install and run unapproved software could exist.
/+/user/ can allow installation of unapproved software.
The assigned user of the mobile computer can be granted
the root account password for /+/user/
Multiple real file systems allow for the possibility
of delegation of authority to separate system administrators.

If not granted an approved method for running unapproved software
then clever users typically circumvent monitoring and rules
by installing and running unapproved software
inside a virtual machine.
The disk file for the virtual machine
would likely reside within the user's home directory
or on the user's personal USB SSD.

However, less than clever users run unapproved software directly.
That grants the software an opportunity
for gaining unauthorized access,
reading user owned files, installing spyware, etc...
And the less than clever use might have a password
or two accidentally stored in clear text within ~/.bash_history
How convenient is that?

Every clever SA knows what each person
is doing with their computers.
When half or more of the staff violates rules
then what becomes the best course of action?
Terminating half the employees
is no more an option than Roman army decimation.
Both tactics create fear and distrust.
Punishment and retribution does not promote
quality nor productivity in a work place.

Above all the SA must maintain the security
of computers for which the SA is responsible.
Therefore, the best course of action
is to plan contingencies
that help promote sufficient security
especially by those violating rules.

People who make rules expect reality to warp
in accordance with print on paper.
In contrast, experienced SAs are always prepared
to investigated, fix, and circumvent
everything which does not go according to plan.
If a problem can not be immediately fixed
then it must be circumvented or mitigated
as best as possible.
That is what skilled SAs must do.

Are virtual computers preferred
to multiple real file systems?
The answer varies.
Security might seem more favorable with virtual computers.
However, immune-security-tomoyo
can help close many security gaps.

Resource utilization is more favorable
when governed by a single Linux kernel.
A single kernel can
more effectively use RAM, SWAP, and block device caching.
A host kernel and guest kernel in a virtual computer
with memory ballooning can not approximate
the efficiency of a single Linux kernel.
The performance costs for using unshared namespace
is considerably less than running para virtual computers.

Unshared namespace allows multiple real file systems
to approximate virtual computing.
Each receives it's own hostname.
Each receives it's own users group and password lists.
Each receives it's own VETH for networking.

If the hostname for the computer is ronin.net
with real file systems such as:
/+/base/ ;
/+/dev/ ;
/+/www/
then the SA might write the hostname for each as:
base.ronin.net; dev.ronin.net; www.ronin.net.

The first line from /etc/hostname becomes the hostname.
Consequently, the hostname of the box can differ.
The hostname specified in /etc/hostname
must be a FQDN, fully qualified domain name.
A DNS "A record" is not required.

Each real file system receives an unshared net namespace.
A VETH, virtual Ethernet adapter, is created.
VETH devices are created in pairs.
Each part of the pair acts like endpoints of a tunnel.
One VETH adapter is moved into each unshared net namespace.
The other VETH in a pair remains in the rootfs net namespace.
This way each VETH can send and receive packets
with the networking stack in the rootfs.
Consequently, the actual network adapters
can remain in the rootfs's net namespace.

IP addresses are assigned to each VETH.
The IP addresses assigned to VETH are
from the non-routable 169.254.1.0 to 169.254.254.255 range.
Those IP addresses are designated
for dynamic assignment in situations
where DHCP is not used.
The network stack in the rootfs
is configured as a NAT, network address translator.
Outbound TCP and UDP connections work without additional configuration.
Inbound connections must be explicitly allowed.

A VETH can be configured with additional IP addresses.
Specify the FQDN and IP address in /etc/hosts
A complimentary route will be created in the rootfs.

The non VETH or ACTUAL network adapters
located in the rootfs net namespace
can be assigned IP addresses by a DHCP server.
However, VETH will never be assigned IP addresses by DHCP.
DHCP running during a runlevel can assign an IP address to a VETH,
yet the complimentary route in the rootfs namespace is NOT created.
Therefore, packets will not route.

/etc/port.forward describes the ports to forward to VETH adapters.
If /+/base/ is already deployed then edit the file as
/+/base/etc/port.forward when logged into /
or as  /etc/port.forward when logged into /+/base/

Each line of /etc/port.forward contain 3 fields.
The field separator can be space or tab.
The first field is a FQDN.
The FQDN must exactly match the content of /+/*/etc/hostname
The second field provides an  inbound port number.
The final field provides the outbound port number
as packets will appear on the VETH.

Consider /+/www/
/etc/port.forward could contain the line:
www.muffins.eat 80 8080

A service running in /+/www/ can bind to port 8080.
And it will receive packets routed to www.muffins.eat:80

Or for simplicity sshd could be enabled with the line:
www.muffins.eat 22 22

Chaning the port number is optional.

Syntax for the file                /etc/port.receive 
is nearly the same as for the file /etc/port.forward
Since the FQDN is already known; delete the first field.
Consequently, /etc/port.receive 
contain only two fields
and a single field separator per line.

For menu driven configuration run:

/etc/init.d/port.forward configure

while logged into /+/base/

Or for menu driven configuration run:

/etc/init.d/port.receive configure

within the a real file system that
wants to request ports to be forward.

Security is the reason
for implementing two separate methods for forwarding ports.
If real file systems should not be allowed to request
ports be forwarded to them
then requests for port forwarding can be ignored
by executing the commands:

chattri -i /+/base/etc/init.d/port.receive
chmod 400  /+/base/etc/init.d/port.receive

while logged into /

As previously mentioned,
static assignment of an IP address to a VETH
automatically grants a complimentary route
from the rootfs network adapters to the VETH.
If done then the VETH adapter should be accessible on the network.
The configured VETH within a real file system
appear as if it were a stand-alone computer on the network.
Port forwarding is not required when a VETH
was assigned a routable IP address.
However, port forwarding still works.

The port forwarding method provides security.
The static assignment of IP address provides convenience.
If an IPv6 address is desired for a VETH
then that IPv6 address must be entered into the /etc/hosts
IP_ADDRESS	hostname	nickname
If the format for /etc/hosts is not already known
then the manual page for it provides the details.
man hosts
The above command displays the manual page for /etc/hosts

The unshared PID namespace creates the illusion
that process processes within a real file system
are the only processes.

Using a combination of unshared
MNT, UTS, IPC, NET and PID namespace
allows each real file system to seem
nearly as if it was it's own computer.
If Linux capabilities are discarded
from the processes in the namespace
or if services for that name space
run in the limited or user tomoyo domains
then escape from the name space
becomes difficult or perhaps impossible.

Obviously, since a single Linux kernel
a certain amount of damage can be accomplished
with EUID 0 and a malicious intent.

The combination of limits by control groups,
installing immune-security-tomoyo,
and possibly dropping Linux capabilities
can help mitigate potential for damage.

Performance and memory sharing
is better than can be achieved with virtual computers.
The security and isolation might not be perfect, yet.
The situation is improving.

Use of unshared namespace was conceived in 2000.
However, the kernel implementation, glibc functions,
and command line tools are new toys for 2012/2013.
Consequently, not all POSIX will be revised
to support multiple real file systems.
The interest may be lacking when
virtual computing provides a similar solution.

Now that the concept file systems
mounted on /+/*/ is understood
the difference between rootfs and /+/base/
should be learned.

rootfs is a combination of TMPFS and RAMFS.
RAMFS is a Random Access Memory File System that exists
entirely in RAM without limits on capacity and inodes.
TMPFS is like a RAMFS because it exists in RAM.
However, the content of TMPFS can be swapped.
And TMPFS is assigned a capacity and inode limit.
The rootfs has all of the characteristics of RAMFS
yet can be swapped like TMPFS.

The purpose of rootfs is to provide a file system
until disk or network based file systems can be mounted.
At that point most POSIX discard the content of their rootfs
and entirely chroot into a single file system.

The rootfs is populated from the content of initramfs images.
initramfs images are cpio archive files.
Begin compressed by gzip or xz is optional.

The content of rootfs is not a clone of /+/base/
Instead portions of /+/base/
are are provided in the initramfs images.
/bin/ /etc/ /init/ /lib/ /lib64/ /sbin/ are typically provided.
However, only useful portions from each are provided.
Only a few files from /usr/ are provided.
/+/base/ might contain between 2 and 8 gigabytes.
However, the rootfs might only contain 200 megabytes.

/+/base/ and rootfs roles differ.
The initramfs images contain files for booting,
trouble shooting, and administrative tasks.
/+/base/ contains the files provided on the rootfs
and also developmental tools for updating installed software.

Since the initramfs images are compressed
about half the size is loaded from disk or network.
After content from the cpio images extract onto the rootfs
then the RAM storing those initramfs images becomes available.
When more RAM becomes required
then most of the rootfs' content
can be swapped to disk.

If the concept of rootfs is not yet clearly understood
then a person might feel tempted to ask the question,
"Does the rootfs require modification or updates?"
Obviously, it does not.
The rootfs exists only in memory.
Consequently, modifications to the rootfs
are discarded during reboot and shutdown.

/+/base/ is the typical origin of the rootfs.
The file system that generates
the initramfs images should be updated and modified.
Modifications made to that file system
become modifications to the rootfs on the following boot.

The final question is probably,
"If rootfs is a shadow of /+/base/
then why have rootfs instead of using /+/base/ ?"

First, the Linux kernel has a size limit.
If the kernel is too large to boot
then it reports that and fails to boot.
Second, compiling some drivers directly into the Linux kernel
causes the Linux kernel to crash.
Third, providing modules allows their size to be reduced.
Forth, modules can be loaded into the kernel when required.
Fifth, modules that fail can be unloaded and re-loaded.
Sixth, the content of RAM required by the kernel
can not be sent to swap.
Therefore, having a minimal selection of modules loaded
allows more RAM to be tasked to processes and caching.

Some POSIX might appear to be able to boot
without using separate initramfs images.
Those Linux kernels have the initramfs image
linked directly into the kernel itself.
A large Linux kernel file
is loaded from disk during booting.
However the POSIX still boots using a rootfs.

Of course a rare exception exists.
Sometimes a SA creates a monolithic kernel configuration.
Such kernels contain compiled in drivers for all the
devices, file systems, protocols, expected to be used on that computer.
Monolithic kernels typically can boot
only the computer for which the kernel was designed.
Few SAs possess the skills to correctly custom craft
a usable monolithic kernel configuration.
POSIX that are designed
to be deployed on a variety of hardware
will not provide monolithic kernels
nor monolithic kernel configurations.
A monolithic kernel configuration
fails to properly boot and shutdown
on a Modern Magic box.

Because a file system must be extracted
from initramfs onto rootfs
then why do other POSIX discard the rootfs?
A typical POSIX creates minimally sized initramfs.
It boots or fails.
Failure requires a DI, Deployment Image.
If the rootfs is successful
then the content of the rootfs is removed
and execution continues after chrooting
into a single file system.

For all the effort and time required
to generate initramfs images
and to load initramfs images on each boot;
why discard the content of the rootfs?
Instead why not fill it with useful programs?
Then the rootfs can be used
to trouble shoot problems.

The rootfs remains a stable platform
for investigating and sniping problems
from a vantage point beyond potential retaliation.
The rootfs can be logged into.
Then all file systems can be unmounted.
File systems can be fscked.
New file systems can be created.
Backups can be restored.
Nearly any type of software related repair
can be accomplished from the rootfs
without rebooting with an DI.

As suggested in the above paragraph,
this POSIX retains the rootfs after booting.
Inactive portions of the rootfs
can be swapped out of RAM.
Consequently, the benefit of retaining the rootfs
costs minimal amounts of RAM.

Booting can fail when less than 1G of RAM is installed.
The actual limit might be closer to 0.5G,
but gradually grows over time.
Sufficient installed RAM is required during booting 
because contents of RAM can not be swapped to disk
until after the swap systems are activated.
The cpio images must be decompressed
and have their content extracted to rootfs
before swap systems can activate.

An exception might exists involving
the passing of a kernel command line parameter.
However, I have not tried it.
It is not described in
/usr/src/linux/Documentation/kernel-parameters.txt

Some of the software projects,
such as MesaLib and firefox,
use about 1 gigabyte of RAM during linking.
Therefore, the amount of RAM required
for software compilation and linking
also serves as an acceptable limit.

Why create smaller less functional initramfs
that are suitable for booting
legacy computers and embedded devices
when such computers lack sufficient RAM
for compiling and linking software?
Consider sacrificing the 75% or more
of the initramfs's functionality
to become able to boot computers
that are incapable of installing software.
Every year people beg me to do this.
I tell them to install more RAM.

If deployment on a legacy computer
or embedded device is desired
then the solution is to deploy
a pre-compiled binary package based POSIX.
Package based POSIX conform to the paradigm
which expects computers to lack
sufficient CPU, RAM, and file system space
that required for software compilation and linking.
That is why those POSIX provide
pre-compiled packages
instead of source tarballs.

Source based POSIX should never be designed
for deployment on computer hardware
that is older than top of the line for 10 years ago.
I do not deploy a workstations
with less than 2 gigabytes of installed RAM.
With only 1G of installed RAM;
users complain about unresponsiveness during updates.
With 2 gigabytes of installed RAM users do not complain.

Wielding the amazing power of an amazing POSIX
requires an amazing computer.
Mediocre POSIX are required for mediocre computers.

Icing enhances the flavor of cake.
How about a vanilla lemon icing on a red building brick?
Does it taste like cake?
Or does the gritty tooth destroying texture
remain less than palatable?

Deploying Modern Magic on a computer 
that has less than 1G installed RAM
is not impossible for a clever SA.
However, the computer will nearly swap lock
as it thrashes for hours while attempting
to link large software projects.
If the device that contains the swap system
is very fast and the computer almost has enough RAM
then linking might only require between
1 and 24 hours where the computer
becomes incapable of doing anything else.
Is that not the metaphorical meal
of gnawing an icing covered red building brick?

Compilation and linking of large software projects
requires at least 1G of RAM.
Some will soon require more than 1G RAM
And more than 1G of RAM is required
for the computer to maintain smooth operation
during software compilation and linking
instead of enduring process suspension during swapping.
Consequently, a work station should have
at least 2G of installed RAM.

The software authors are responsible
for the amount of RAM required for compilation and linking.
Therefore, computers that are incapable
of software compilation and linking
should deploy legacy package based POSIX.
Packages provide software
that is already compiled and linked.
This POSIX does not provide packages,
because it is source based.
