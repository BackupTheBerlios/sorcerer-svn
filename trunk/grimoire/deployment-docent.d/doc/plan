Plans determine how mass storage devices are utilized.
A fixed disk is an example of a mass storage device.
A solid state device, SSD, of size 64G of larger
is also considered a mass storage device.

Mass storage devices require preparation.
Partitioning is typical preparation.
Partitions desginated areas.
Each partition is delegated a use.
A partition or suitable block device
is utilized for a file system, swap area, or database.
A reasonably simple deployment requires 5 partitions.

Simply Workstation Plan:
Part #	Size		Type	Mount Point
1	1G		ext2	/+/base/boot/
2	1G		swap
3	8G		ext4	/+/base/
4	30G		ext4	/+/work/
5	Varies		ext4	/+/home/

Partition numbers specify order of creation.
The boot partition should occur first.
What is now a tradition was historically necessary.

/+/base/ is also called the foundation file system.
The primary purpose is to provide the cpio initramfs images
used to populate the rootfs during booting.
However, the terse manifest of installed software grants additional boons.

Other root file systems can be updated more frequently.
/+/base/ updates faster than othe root file systems.
/+/base/ can be updated prior to reboot.

/+/base/ can be copied to a USB SSD and made bootable.
/+/base/ on USB SSD can be used to deploy and rescue.
/+/base/ can be used to rescue the deployed computer.
Installed software on the I/R eventually becomes too old
to rescue a deployed and updated computer.

/+/base/ installed software should always be sufficient 
for accomplishing the tasks of updating and booting.
However, some of the installed software
is not necessary for other root file systems.
File systems smaller than base are atypical.
A server root file system might be smaller.
A workstation root file system requires more space.

New system administrators can
omit reading the remainder of this document.
Definitions for vernacular in this document
from this point forward is not provided.
The following plans are more sophisticated.
Successful prior deployment is required.

Order of Device Initialization:

Partitions       can be designated physical volumes.
Partitions       can be joined into a mdRAID device.
mdRAID devices   can be desginated physical volumes.
mdRAID devices   can be partitioned.
Physical volumes can be joined to     volume groups.
Logical  volumes can be carved from a volume group.
A LUKS device    can be created from:
a partition; a raid device; a logical volume.

Variation is possible.
Partitioning a mdRAID to provide /boot/ never works.
Instead partition each mass storage device.
Join partitions into mdRAID devices.
A mdRAID for /boot/ must be type RAID 1.
Only type RAID 1 is bootable.

Other partitions can be joined
in types RAID 2, 3, 4, 5, 6, 10
The resultant mdRAID can be partitioned.
Or it can be made into a physical volume.

Physical volumes can be added to volume group.
Logical  volumes can be carved from a volume group.
Logical  volumes are suitable for a file system, swap area, or database.

Utilization of RAID or LVM2 with inadequate understanding
may result in deployment that is unable to boot.
Use of a file system other than ext2 for /boot/
may result in file system that corrupts
and fails to boot soon after deployment.
ext2 provides a reserved boot block.
XFS does not provide a reserved boot block.
Read file system documention prior to creating a plan
that utilizes diferent file systems than are recommended.


Suspendable Plan:
Part #	Size	Type	Mount Point
3	8G		ext4	/+/base/
1	1G		ext2	/+/base/boot/
2	RAM/2		swap
4	30G		ext4	/+/work/
5	Varies		ext4	/+/home/

Dedicated swap partitions provide better performance for less RAM.
Performance is further enhanced by activating swap partitions
that reside on a disk that does not host root file systems.

Swap files may become fragmented faster than swap partitions.
Swap files require more RAM.
However, files can be automatically created and activited upon demand.

A single adequately sized swap partition is mandatory
if a computer should be able to suspend to disk.
Suspend to disk involves writing RAM to disk before halting.
On reboot content is read from the swap partition into RAM.

Suspend to disk and resume can not be utilized
after the installed Linux on /+/boot/ was updated.
Attempting to do so will cause loss of information
and the possibility of file system corruption.
Therefore, rebooting after /+/base/ updates
is always a good idea and mandatory for a computer
which can suspend to disk.

Where BIOS is adequate for suspend to disk;
suspend to RAM is also possible.
If power is interrupted or batteries become depleted
then resume becomes impossible.
Data will be locst and file system corruption may result.

A mobile computer that stores only unimportant information
is ideal for suspend and resume.
Suspend and resume should not be used on servers.

Secure Mobility Plan:
Part #	Size		Type		Mount Point
3	8G		LUKS ext4	/+/base/
1	1G		ext2		/+/base/boot/
2	RAM/2		LUKS swap
4	30G		LUKS ext4	/+/work/
5	Varies		LUKS ext4	/+/home/


The Secure Mobility Plan resembles the Suspendable plan.
However, this plan can not be suspended because
the encryption keys for swap are stored on swap
which is an encrypted partition.
An unencrypted swap defeats the security
of the Secure Mobility Plan
even when the computer is never suspended to disk.

If /+/base/boot/ is created on a USB SSD
then it can be worn carried with the person
and stored in a safe when not in use.
A secure computer with no unencrypted partitions
in a powered down state is secuerd against data theft.

The two points of vulnerability are
the operator and the potential for intrusion
when attached to a network.
If the USB SSD is sized
then it can be trojaned and returned.
But without passphrases the USB SSD alone
can not be utilized to mount partitions
on LUKS devices.

To further harden the computer installation
of immune-system-tomoyo is recommended.
Until able to transition out of a restricted tomoyo domain
then access to system areas of root file systems
remains read only all EUID including EUID 0.


Performance Workstation Plan:
Part #	Size		Type		Mount Point
3	8G		ext4		/+/base/
1	1G		ext2		/+/base/boot/
2	1G		swap
4	5G		ext4		/tmp/
5	60G		ext4		/aux/can/
6	20G		ext4		/aux/run/
7	20G		ext4		/+/work/
8	Varies		LUKS ext4	/+/work/home/

Binds

From			To
/aux/can/		/+/base/aux/can/
/aux/can/		/+/work/aux/can/
/aux/can/run/base/	/+/base/aux/run/
/aux/can/run/work/	/+/work/aux/run/
/tmp/base/		/+/base/tmp/
/tmp/work/		/+/work/tmp/

The binds allow /aux/can/source/
to be shared by all the root file systems.
Each root file system also gains private use of /tmp/ and /aux/run/

The large size of /aux/can/ permits
an extensive collection of source tarballs.
Currently, 40G might suffice,
but each year the amount grows.

Currently, 20G for /+/work/ sufficies for installing
xorg, firefox, and one desktop environment.
For a deluxe manifest of software increase the size to 30G.
For adequate space for installing game software
increase the size to 40G.

Or slightly more space can be used to provide
/+/user/ or /+/play/
that allows for software to be installed
that might be developmental versions
or not approved for employee use.

Optionally the swap system can be created
on a LUKS encrypted device
using a pass phrase that is randomly generated on each boot.
This is acceptable because a computer with a LUKS encrypted device
should never be suspended to disk, because that would cause
the encryption key to be written to the swap space.

Multi-User Laptop Plan:
Part #	Size		Type		Mount Point
2	8G		LUKS ext4	/+/base/
1	1G		ext2		/+/base/boot/
3	40G		LUKS ext4	/+/user1/
4	40G		LUKS ext4	/+/user2/
5	40G		LUKS ext4	/+/user3/


The above plan provides less performance.
Each user receives their own /+/base/boot/ on USB SSD.
Each LUKS device receives a unique password.
Each user receives one passphrase that unlocks
only one of the user root file systems.
Therefore, each users's /home/ files and root file systems remain.

Large capacity USB SSD sacrifice performance.
However, entire deployment can be made
to a USB SSD of size 32G or larger.
The SA can always ssh into the laptop while it is on
in order to update the active USB key
and active root file system.
However, local login should be used for updating /+/base/


Corporate Workstation Plan:
Part #	Size		Type		Mount Point
3	8G		ext4		/+/base/
1	1G		ext2		/+/base/boot/
2	1G		swap
4	5G		ext4		/tmp/
5	60G		ext4		/aux/can/
6	20G		ext4		/aux/run/
7	30G		ext4		/+/work/
8	40G		ext4		/+/user/


This plan differs slightly from the Performance Workstation Plan.
/+/user/ becomes an additional root file system.
The regular user of the workstation is granted
the root password for the /+/user/ root file system.
Permission is also granted to install software projects.
This way the company policy software remains installed in /+/work/
while the user preferred software is         installed in /+/user/.

Those higher in managment hierarchy than system administrator
fail to comprehend the reason for a /+/user/ root file system.
Users violate company policy
by installing software without permission
and using a computer for unauthorized activity.
Users will install software within their home directory;
run a virtual computer from a file within their home directory;
boot Live CDs POSIX;
boot Live USB POSIX.

Due to sloppiness and ignorance
non work related activities can compromise security.
Better to compromise the /+/user/ root file system
than   to compromise the /+/work/ root file system.

Providing, a separate root file system
for the user to log into
for non work related activity
helps mitigate threat of intrusion and data theft.

/+/work/home/ and
/+/user/home/ must never be bound together.
That mitigates intrusion into /+/work/
by a program that originally implanted within the /+/user/home/ hierarchy.
 
Adapt any aforementioned plan to create the mdRAID Plan.
Each disk should have a 1G /boot/ partition
that is joined in a mdRAID 1
Other partitions can be joined in any type of mdRAID desired.
The non /boot/ mdRAID device can be partitioned
or converted to a physical volume.

LVM2 Plan:
1	1G		ext2	/+/base/boot/
2	variable	LVM2	mdRAID or partition converted to physical volume

Adapt any aforementioned plant to create a LVM2 plan.
Join the physical volume to a volume group.
Carve logcal volume block devices from the volume group.

If utilizing LVM2 on a single disk
then a dedicated /boot/ partition is mandatory.
Linux understands LVM2.
Bootloaders do not understand LVM2.

Logical volumes are conceptually like partitions.
However, the implementation entirely differs.
A partition is defined in the partition table.
A partition utilizes only continuous space on a device.

Logical volumes are mapped
from any available space a volume group can provide.
Fragmentation of a logical volume
incurs the same performance penalty of fragmented files.
Space within a volume group can be mapped
or unmapped to any number of logical volumes.
A volume group can provide an ideal scratch
pad for playing with file systems that
are intended to be eventually discarded.

Considerations for Plan Selection:

LVM2   is an implementation specific to the Linux kernel.
mdRAID is an implementation specific to the Linux kernel.
LUKS   is an implementation specific to the Linux kernel.
Partition tables and partitions
can be read by various operating systems.

A nearly infinite amount of plans can be envisioned.
No single plan is ideal.
Some plans are more advantageous.
Some plans are predetermined failures.
The following gives demonstration of FAIL plans.

Fail to Read File System Documentation Plan 1:
1	maximum		xfs	/+/base/

Why not install everything on a single partition
instead of doing a multi-root installation?
Deployment seems smooth.
But the computer fails on either the first or second boot.
Why does this plan fail?
According to the XFS documentation;
XFS lacks a reserved boot block.
Therefore, installing the boot loader
to the boot block of / on XFS
corrupts the file system.

Failure to Partition Plan 2:
/dev/sda	maximum		ext4	/+/base/

Some people claim that this works.
If the boot loader is installed to /dev/sda will it work?
A leviathan for a root file system severely impacts performance.


Tiny /boot Partition Plan:
2	maximum		ext3	/+/base/
1	16M		ext2	/+/base/boot/

It worked perfectly in 1990, why not now?
A POSIX that ceased updating in 1990
is suitable for deployment on a computer from 1990.
Deploying modern software requires
appropriate utilization of modern hardware.

Modifying recommended plans incur the peril of inducing failures.
Deviate from recommended plans when the consequence is known.
Prudent modifications involve adding more and increasing sizes.

Dual or multi-booting is not recommended.
If ignoring then at least deploy each to different disks.
If ignoring then sob when the MBR is auto-annexed.

No warranty is provided.
Be careful.
Make backups.

Deploy to a quem virtual machine for a test drive.
Every plan's performance benefits are mitigated
when deployed on a virtual computer.
The Simplicity Plan is suitable.
Boot the virtual computer using HVM.
If PV booting is desired then ask on qemu's email list or message forums.

Visit the qemu website for extensive documentation
that describe how to create virtual disk files
and the command line parameters for launching qemu.
http://wiki.qemu.org/
